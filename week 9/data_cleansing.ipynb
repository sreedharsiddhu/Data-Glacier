{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Cleansing and Transformation\n",
                "**Team:** The Closer  \n",
                "**Member:** Rongala Sreedhar  \n",
                "\n",
                "## Problem Description\n",
                "The goal is to prepare the Healthcare dataset for analysis by addressing data quality issues. We will demonstrate techniques for:\n",
                "1. Handling Missing Values (Imputation)\n",
                "2. Handling Outliers\n",
                "3. NLP Featurization and Cleaning\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.impute import KNNImputer\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "import re\n",
                "\n",
                "# Load Data\n",
                "df = pd.read_excel('Healthcare_dataset (1).xlsx', sheet_name='Dataset')\n",
                "print(\"Original Shape:\", df.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Corruption (Simulation)\n",
                "Since the dataset has 0 missing values, we introduce synthetic NaNs to demonstrate our cleaning techniques."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "df_corrupted = df.copy()\n",
                "cols_to_corrupt = ['Dexa_Freq_During_Rx', 'Count_Of_Risks']\n",
                "\n",
                "for col in cols_to_corrupt:\n",
                "    mask = np.random.choice([True, False], size=df.shape[0], p=[0.1, 0.9])\n",
                "    df_corrupted.loc[mask, col] = np.nan\n",
                "    \n",
                "print(\"Missing values after corruption:\")\n",
                "print(df_corrupted[cols_to_corrupt].isna().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Approach 1: Simple Imputation & Outlier Removal\n",
                "**Member:** Rongala Sreedhar  \n",
                "**Technique:** Mean/Mode Imputation + IQR Outlier Removal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for approach 1\n",
                "df_app1 = df_corrupted.copy()\n",
                "\n",
                "# Impute Numeric with Median (Count_Of_Risks)\n",
                "median_val = df_app1['Count_Of_Risks'].median()\n",
                "df_app1['Count_Of_Risks'].fillna(median_val, inplace=True)\n",
                "\n",
                "# Impute Categorical with Mode (Dexa_Freq_During_Rx)\n",
                "mode_val = df_app1['Dexa_Freq_During_Rx'].mode()[0]\n",
                "df_app1['Dexa_Freq_During_Rx'].fillna(mode_val, inplace=True)\n",
                "\n",
                "# Outlier Removal using IQR (on Count_Of_Risks)\n",
                "Q1 = df_app1['Count_Of_Risks'].quantile(0.25)\n",
                "Q3 = df_app1['Count_Of_Risks'].quantile(0.75)\n",
                "IQR = Q3 - Q1\n",
                "lower = Q1 - 1.5 * IQR\n",
                "upper = Q3 + 1.5 * IQR\n",
                "\n",
                "df_clean_1 = df_app1[(df_app1['Count_Of_Risks'] >= lower) & (df_app1['Count_Of_Risks'] <= upper)]\n",
                "print(f\"Approach 1: Shape after cleaning: {df_clean_1.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Peer Review (Simulated)\n",
                "> **Reviewer:** Example Peer\n",
                "> *\"Good use of Median for count data as it's less sensitive to outliers. IQR is standard but aggressive; check if we lost too many rows.\"*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Approach 2: Model-Based Imputation & Transformations\n",
                "**Member:** (Peer / Alternative Approach)  \n",
                "**Technique:** KNN Imputation + Log Transformation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for approach 2\n",
                "df_app2 = df_corrupted.copy()\n",
                "\n",
                "# KNN Imputation requires numeric encoding usually, but we'll demonstrate on numeric col\n",
                "imputer = KNNImputer(n_neighbors=5)\n",
                "df_app2[['Count_Of_Risks']] = imputer.fit_transform(df_app2[['Count_Of_Risks']])\n",
                "\n",
                "# Handle Categorical Missing (Dexa_Freq_During_Rx) with 'Missing' category\n",
                "df_app2['Dexa_Freq_During_Rx'].fillna('Missing_Data', inplace=True)\n",
                "\n",
                "# Outlier Handling: Log Transformation (instead of removal)\n",
                "# Adding 1 to avoid log(0)\n",
                "df_app2['Log_Count_Risks'] = np.log1p(df_app2['Count_Of_Risks'])\n",
                "\n",
                "print(f\"Approach 2: Shape retained: {df_app2.shape}\")\n",
                "print(\"Log Transform Skewness:\", df_app2['Log_Count_Risks'].skew())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. NLP Featurization and Cleaning\n",
                "**Target Column:** `Ntm_Speciality`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_data = df['Ntm_Speciality'].astype(str)\n",
                "\n",
                "# Regex Cleaning function\n",
                "def clean_text(text):\n",
                "    text = text.lower()\n",
                "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove punctuation\n",
                "    return text\n",
                "\n",
                "df['Clean_Speciality'] = text_data.apply(clean_text)\n",
                "\n",
                "# Featurization 1: Count Vectorizer\n",
                "cv = CountVectorizer(stop_words='english')\n",
                "cv_matrix = cv.fit_transform(df['Clean_Speciality'])\n",
                "print(\"Count Vectorizer Shape:\", cv_matrix.shape)\n",
                "\n",
                "# Featurization 2: TF-IDF\n",
                "tfidf = TfidfVectorizer(max_features=10)\n",
                "tfidf_matrix = tfidf.fit_transform(df['Clean_Speciality'])\n",
                "print(\"TF-IDF Shape:\", tfidf_matrix.shape)\n",
                "print(\"Top TF-IDF features:\", tfidf.get_feature_names_out())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}